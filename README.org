#+title: Azuma ANN RNN
* Introduction
I just want to use *LISP* in order to manage a simple /ANN/ and finally transform this one in a pretty simple /RNN/  
* ANN
** Scenario azuma 1
#+begin_src emacs-lisp
(push #P"project_directory" asdf:*central-registry*) ; If the project is outside 

(ql:quickload "azuma") ; ou (asdf:load-system "cpu-mem-mlp")
(in-package :azuma)

(defparameter *net* (make-mlp-2-4-2))

(defparameter *xs*
  (vector
   #(0.4 0.5 0.6 0.3)
   #(0.2 0.1 0.7 0.9)
   #(0.1 0.9 0.3 0.4)
   #(0.6 0.2 0.5 0.8)))

(defparameter *ys*
  (vector
   #(0.5 0.4)
   #(0.8 0.2)
   #(0.3 0.7)
   #(0.9 0.1)))

(defparameter *loss-history*
  (train-epochs *net* *xs* *ys*
                :epochs 50
                :batch-size 2
                :learning-rate 0.05))

(export-loss-history-to-dat *loss-history* "datas/loss_epochs.dat")
#+end_src

** Scenario azuma 2
#+begin_src emacs-lisp
(ql:quickload "cpu-mem-mlp") ; ou (asdf:load-system "cpu-mem-mlp")
(in-package :cpu-mem-mlp)

(defparameter *net* (make-mlp-2-4-2))

(defparameter *xs*
  (vector
   #(0.4 0.5 0.6 0.3)
   #(0.2 0.1 0.7 0.9)
   #(0.1 0.9 0.3 0.4)
   #(0.6 0.2 0.5 0.8)))

(defparameter *ys*
  (vector
   #(0.5 0.4)
   #(0.8 0.2)
   #(0.3 0.7)
   #(0.9 0.1)))

(defparameter *loss-history*
  (train-epochs *net* *xs* *ys*
                :epochs 50
                :batch-size 2
                :learning-rate 0.05))

(export-loss-history-to-dat *loss-history* "loss_epochs.dat")
#+end_src

** Loss evolution
#+begin_src gnuplot :file datas/loss_epochs.png :results output graphics file :exports results
set xlabel "Epoch"
set ylabel "Loss moyenne"
set grid

plot "datas/loss_epochs.dat" using 1:2 with linespoints title "Training loss"
#+end_src
* RNN
** Scenario azuma01

#+begin_src emacs-lisp
(ql:quickload "cpu-mem-mlp")
(in-package :cpu-mem-mlp)

(defparameter *rnn* (make-rnn-network-2-4-2))

;; Exemple : chaque séquence = 3 pas de temps, chaque x_t = #(cpu mem)
(defparameter *seq1*
  (vector
   #(0.4 0.5)
   #(0.6 0.3)
   #(0.7 0.4)))

(defparameter *seq2*
  (vector
   #(0.2 0.1)
   #(0.3 0.2)
   #(0.5 0.4)))

;; Targets (CPU/MEM à t+1 par ex.)
(defparameter *ys*
  (vector
   #(0.8 0.5)
   #(0.6 0.3)))

(defparameter *seqs*
  (vector *seq1* *seq2*))

(defparameter *rnn-loss-history*
  (rnn-train-epochs *rnn* *seqs* *ys*
                    :epochs 50
                    :learning-rate 0.01))
#+end_src

** Loss evolution
#+begin_src gnuplot :file datas/rnn_epochs.png :results output graphics file :exports results
set xlabel "Epoch"
set ylabel "Loss moyenne"
set grid

plot "datas/rnn_loss.dat" using 1:2 with linespoints title "RNN training loss"
#+end_src

#+RESULTS:
[[file:datas/rnn_epochs.png]]
* Prediction
** Resume with the MLP
We toke example on 2 hours
#+begin_src emacs-lisp
x = [CPU(t-1) MEM(t-1) CPU(t) MEM(t)]
y_pred = [CPU(t+1) MEM(t+1)]
#+end_src
+ Imagine that we build an input vector such as:
  #+begin_src emacs-lisp
;; last hours known
(defparameter *cpu-t-1* 0.4)
(defparameter *mem-t-1* 0.5)
(defparameter *cpu-t*   0.6)
(defparameter *mem-t*   0.3)

(defparameter *x-new*
  (vector *cpu-t-1* *mem-t-1* *cpu-t* *mem-t*))
;; => #(0.4 0.5 0.6 0.3)
  #+end_src
We suppose the network trained
#+begin_src emacs-lisp
(defparameter *net* (make-mlp-2-4-2))
;; ... puis tu l'as entraîné avec train-epochs ...
#+end_src
So the prediction can be done like this:
#+begin_src emacs-lisp
(defparameter *y-pred* (forward *net* *x-new*))
;; => par ex. #(0.72 0.665) avant entraînement
#+end_src
and finally
#+begin_src emacs-lisp
(aref *y-pred* 0)  ;; prédiction CPU(t+1)
(aref *y-pred* 1)  ;; prédiction MEM(t+1)
#+end_src
Obviously that values are standardized, that why we'll have to denormalize in % for example.
** Resume with the RNN
Now, our input is based on a huge vector with sequency of vectors (cpu, mem)
#+begin_src emacs-lisp
seq-x = [ x₁, x₂, ..., x_T ]
x_t = [CPU(t), MEM(t)]
#+end_src
and the output prediction based on the sequency.
So, imagine for the last 3 hours.
#+begin_src emacs-lisp
;; t-2, t-1, t (standardized)
(defparameter *seq-x*
  (vector
   #(0.4 0.5)  ; CPU(t-2), MEM(t-2)
   #(0.6 0.3)  ; CPU(t-1), MEM(t-1)
   #(0.7 0.4))) ; CPU(t), MEM(t)
#+end_src
Obviously that the network is trained,
#+begin_src emacs-lisp
(defparameter *rnn* (make-rnn-network-2-4-2))
;; ... puis entraîné avec rnn-train-epochs ...
#+end_src
The prediction method is pretty simple:
#+begin_src emacs-lisp
(defparameter *y-rnn-pred*
  (rnn-forward-seq *rnn* *seq-x*))
;; => par ex. #(0.65 0.55) (valeurs fictives)
#+end_src
and
#+begin_src emacs-lisp
(aref *y-rnn-pred* 0)  ;; CPU(t+1) prédit
(aref *y-rnn-pred* 1)  ;; MEM(t+1) prédit
#+end_src
We don't forget to denormalize the result.

** Graphic
#+begin_src plantuml :file datas/diagram.svg :results silent :exports none 
@startuml NeuralNetwork

skinparam monochrome false
skinparam shadowing false
skinparam defaultFontName Arial
left to right direction

skinparam rectangle<<behavior>> {
	roundCorner 25
}

rectangle "Input 1" as input1 #ADD1B2
rectangle "Input 4" as input4 #ADD1B2
rectangle "Input 3" as input3 #ADD1B2
rectangle "Input 2" as input2 #ADD1B2

usecase "Neuron 1\n sum + act" as hidden1
usecase "Neuron 2\n sum + act" as hidden2

rectangle "Output 1" as output1 #A9DCDF
rectangle "Output 2" as output2 #A9DCDF

input1 --> hidden1 #text:blue : w11
input2 --> hidden1 #text:blue : w12
input3 --> hidden1 #text:blue : w13
input4 --> hidden1 #text:blue : w14

input1 --> hidden2 #text:blue : w21
input2 --> hidden2 #text:blue : w22
input3 --> hidden2 #text:blue : w23
input4 --> hidden2 #text:blue : w24

hidden1 --> output1
hidden2 --> output1
hidden1 --> output2
hidden2 --> output2

@enduml
#+end_src

[[file:datas/diagram.svg]]
